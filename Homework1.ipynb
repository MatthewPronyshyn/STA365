{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4413d705",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "# Matthew Pronyshyn 1002365978\n",
    "\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "062f2946-204a-4dfb-ae77-37e2ca6d2a43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After trial 1, bandit Other: Alpha 1.0, Beta 2.0, Expectation 0.33\n",
      "After trial 2, bandit Blue: Alpha 2.0, Beta 1.0, Expectation 0.67\n",
      "After trial 3, bandit Blue: Alpha 3.0, Beta 1.0, Expectation 0.75\n",
      "After trial 4, bandit Blue: Alpha 3.0, Beta 2.0, Expectation 0.6\n",
      "After trial 5, bandit Blue: Alpha 4.0, Beta 2.0, Expectation 0.67\n",
      "After trial 6, bandit Blue: Alpha 5.0, Beta 2.0, Expectation 0.71\n",
      "After trial 7, bandit Blue: Alpha 5.0, Beta 3.0, Expectation 0.62\n",
      "After trial 8, bandit Blue: Alpha 6.0, Beta 3.0, Expectation 0.67\n",
      "After trial 9, bandit Blue: Alpha 7.0, Beta 3.0, Expectation 0.7\n",
      "After trial 10, bandit Blue: Alpha 8.0, Beta 3.0, Expectation 0.73\n",
      "After trial 11, bandit Blue: Alpha 8.0, Beta 4.0, Expectation 0.67\n",
      "After trial 12, bandit Red: Alpha 1.0, Beta 2.0, Expectation 0.33\n",
      "After trial 13, bandit Other: Alpha 1.0, Beta 3.0, Expectation 0.25\n",
      "After trial 14, bandit Red: Alpha 1.0, Beta 3.0, Expectation 0.25\n",
      "After trial 15, bandit Blue: Alpha 9.0, Beta 4.0, Expectation 0.69\n",
      "After trial 16, bandit Red: Alpha 1.0, Beta 4.0, Expectation 0.2\n",
      "After trial 17, bandit Blue: Alpha 9.0, Beta 5.0, Expectation 0.64\n",
      "After trial 18, bandit Blue: Alpha 10.0, Beta 5.0, Expectation 0.67\n",
      "After trial 19, bandit Other: Alpha 2.0, Beta 3.0, Expectation 0.4\n",
      "After trial 20, bandit Other: Alpha 3.0, Beta 3.0, Expectation 0.5\n",
      "After trial 21, bandit Blue: Alpha 10.0, Beta 6.0, Expectation 0.62\n",
      "After trial 22, bandit Other: Alpha 3.0, Beta 4.0, Expectation 0.43\n",
      "After trial 23, bandit Blue: Alpha 10.0, Beta 7.0, Expectation 0.59\n",
      "After trial 24, bandit Blue: Alpha 10.0, Beta 8.0, Expectation 0.56\n",
      "After trial 25, bandit Blue: Alpha 10.0, Beta 9.0, Expectation 0.53\n",
      "After trial 26, bandit Blue: Alpha 11.0, Beta 9.0, Expectation 0.55\n",
      "After trial 27, bandit Other: Alpha 4.0, Beta 4.0, Expectation 0.5\n",
      "After trial 28, bandit Blue: Alpha 12.0, Beta 9.0, Expectation 0.57\n",
      "After trial 29, bandit Blue: Alpha 12.0, Beta 10.0, Expectation 0.55\n",
      "After trial 30, bandit Blue: Alpha 13.0, Beta 10.0, Expectation 0.57\n",
      "After trial 31, bandit Blue: Alpha 14.0, Beta 10.0, Expectation 0.58\n",
      "After trial 32, bandit Blue: Alpha 15.0, Beta 10.0, Expectation 0.6\n",
      "After trial 33, bandit Blue: Alpha 15.0, Beta 11.0, Expectation 0.58\n",
      "After trial 34, bandit Other: Alpha 5.0, Beta 4.0, Expectation 0.56\n",
      "After trial 35, bandit Blue: Alpha 16.0, Beta 11.0, Expectation 0.59\n",
      "After trial 36, bandit Other: Alpha 6.0, Beta 4.0, Expectation 0.6\n",
      "After trial 37, bandit Other: Alpha 7.0, Beta 4.0, Expectation 0.64\n",
      "After trial 38, bandit Other: Alpha 8.0, Beta 4.0, Expectation 0.67\n",
      "After trial 39, bandit Blue: Alpha 16.0, Beta 12.0, Expectation 0.57\n",
      "After trial 40, bandit Other: Alpha 8.0, Beta 5.0, Expectation 0.62\n",
      "After trial 41, bandit Other: Alpha 9.0, Beta 5.0, Expectation 0.64\n",
      "After trial 42, bandit Other: Alpha 10.0, Beta 5.0, Expectation 0.67\n",
      "After trial 43, bandit Other: Alpha 10.0, Beta 6.0, Expectation 0.62\n",
      "After trial 44, bandit Other: Alpha 10.0, Beta 7.0, Expectation 0.59\n",
      "After trial 45, bandit Other: Alpha 11.0, Beta 7.0, Expectation 0.61\n",
      "After trial 46, bandit Other: Alpha 12.0, Beta 7.0, Expectation 0.63\n",
      "After trial 47, bandit Other: Alpha 13.0, Beta 7.0, Expectation 0.65\n",
      "After trial 48, bandit Other: Alpha 14.0, Beta 7.0, Expectation 0.67\n",
      "After trial 49, bandit Other: Alpha 15.0, Beta 7.0, Expectation 0.68\n",
      "After trial 50, bandit Red: Alpha 2.0, Beta 4.0, Expectation 0.33\n",
      "After trial 51, bandit Other: Alpha 15.0, Beta 8.0, Expectation 0.65\n",
      "After trial 52, bandit Other: Alpha 16.0, Beta 8.0, Expectation 0.67\n",
      "After trial 53, bandit Blue: Alpha 16.0, Beta 13.0, Expectation 0.55\n",
      "After trial 54, bandit Other: Alpha 17.0, Beta 8.0, Expectation 0.68\n",
      "After trial 55, bandit Other: Alpha 18.0, Beta 8.0, Expectation 0.69\n",
      "After trial 56, bandit Other: Alpha 19.0, Beta 8.0, Expectation 0.7\n",
      "After trial 57, bandit Other: Alpha 20.0, Beta 8.0, Expectation 0.71\n",
      "After trial 58, bandit Other: Alpha 21.0, Beta 8.0, Expectation 0.72\n",
      "After trial 59, bandit Red: Alpha 3.0, Beta 4.0, Expectation 0.43\n",
      "After trial 60, bandit Red: Alpha 4.0, Beta 4.0, Expectation 0.5\n",
      "After trial 61, bandit Other: Alpha 22.0, Beta 8.0, Expectation 0.73\n",
      "After trial 62, bandit Other: Alpha 23.0, Beta 8.0, Expectation 0.74\n",
      "After trial 63, bandit Other: Alpha 24.0, Beta 8.0, Expectation 0.75\n",
      "After trial 64, bandit Other: Alpha 25.0, Beta 8.0, Expectation 0.76\n",
      "After trial 65, bandit Red: Alpha 5.0, Beta 4.0, Expectation 0.56\n",
      "After trial 66, bandit Other: Alpha 26.0, Beta 8.0, Expectation 0.76\n",
      "After trial 67, bandit Other: Alpha 26.0, Beta 9.0, Expectation 0.74\n",
      "After trial 68, bandit Other: Alpha 27.0, Beta 9.0, Expectation 0.75\n",
      "After trial 69, bandit Red: Alpha 5.0, Beta 5.0, Expectation 0.5\n",
      "After trial 70, bandit Other: Alpha 28.0, Beta 9.0, Expectation 0.76\n",
      "After trial 71, bandit Other: Alpha 28.0, Beta 10.0, Expectation 0.74\n",
      "After trial 72, bandit Red: Alpha 5.0, Beta 6.0, Expectation 0.45\n",
      "After trial 73, bandit Other: Alpha 28.0, Beta 11.0, Expectation 0.72\n",
      "After trial 74, bandit Other: Alpha 28.0, Beta 12.0, Expectation 0.7\n",
      "After trial 75, bandit Other: Alpha 29.0, Beta 12.0, Expectation 0.71\n",
      "After trial 76, bandit Other: Alpha 30.0, Beta 12.0, Expectation 0.71\n",
      "After trial 77, bandit Other: Alpha 30.0, Beta 13.0, Expectation 0.7\n",
      "After trial 78, bandit Other: Alpha 30.0, Beta 14.0, Expectation 0.68\n",
      "After trial 79, bandit Other: Alpha 31.0, Beta 14.0, Expectation 0.69\n",
      "After trial 80, bandit Blue: Alpha 17.0, Beta 13.0, Expectation 0.57\n",
      "After trial 81, bandit Other: Alpha 32.0, Beta 14.0, Expectation 0.7\n",
      "After trial 82, bandit Other: Alpha 33.0, Beta 14.0, Expectation 0.7\n",
      "After trial 83, bandit Blue: Alpha 18.0, Beta 13.0, Expectation 0.58\n",
      "After trial 84, bandit Other: Alpha 34.0, Beta 14.0, Expectation 0.71\n",
      "After trial 85, bandit Other: Alpha 35.0, Beta 14.0, Expectation 0.71\n",
      "After trial 86, bandit Other: Alpha 36.0, Beta 14.0, Expectation 0.72\n",
      "After trial 87, bandit Other: Alpha 37.0, Beta 14.0, Expectation 0.73\n",
      "After trial 88, bandit Other: Alpha 38.0, Beta 14.0, Expectation 0.73\n",
      "After trial 89, bandit Other: Alpha 38.0, Beta 15.0, Expectation 0.72\n",
      "After trial 90, bandit Other: Alpha 39.0, Beta 15.0, Expectation 0.72\n",
      "After trial 91, bandit Other: Alpha 40.0, Beta 15.0, Expectation 0.73\n",
      "After trial 92, bandit Other: Alpha 40.0, Beta 16.0, Expectation 0.71\n",
      "After trial 93, bandit Other: Alpha 41.0, Beta 16.0, Expectation 0.72\n",
      "After trial 94, bandit Other: Alpha 42.0, Beta 16.0, Expectation 0.72\n",
      "After trial 95, bandit Other: Alpha 42.0, Beta 17.0, Expectation 0.71\n",
      "After trial 96, bandit Other: Alpha 43.0, Beta 17.0, Expectation 0.72\n",
      "After trial 97, bandit Other: Alpha 43.0, Beta 18.0, Expectation 0.7\n",
      "After trial 98, bandit Other: Alpha 44.0, Beta 18.0, Expectation 0.71\n",
      "After trial 99, bandit Other: Alpha 45.0, Beta 18.0, Expectation 0.71\n",
      "After trial 100, bandit Other: Alpha 45.0, Beta 19.0, Expectation 0.7\n",
      "True/Posterior expectations: \"Red\" 0.25/0.45, \"Blue\" 0.5/0.58, \"Other\" 0.75/0.7\n",
      "Bandit \"Other\" has the highest posterior expectation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Number of bandits\n",
    "num_bandits = 3\n",
    "bandit_names = ['Red', 'Blue', 'Other']  # Names for the bandits\n",
    "\n",
    "# Fixed probabilities of success for each bandit\n",
    "true_probabilities = {\n",
    "    'Red': 0.25,  # 25% chance of success\n",
    "    'Blue': 0.5,  # 50% chance of success\n",
    "    'Other': 0.75  # 75% chance of success\n",
    "}\n",
    "\n",
    "# Prior parameters for the Beta distribution (uninformative uniform prior)\n",
    "alpha = np.ones(num_bandits)\n",
    "beta = np.ones(num_bandits)\n",
    "\n",
    "# Update function for posterior\n",
    "def update_posterior(bandit_index, result):\n",
    "    if result: # If the result was a success\n",
    "        alpha[bandit_index] += 1\n",
    "    else: # If the result was a failure\n",
    "        beta[bandit_index] += 1\n",
    "\n",
    "# Choose bandit function\n",
    "def choose_bandit():\n",
    "    sampled_theta = [stats.beta.rvs(a, b) for a, b in zip(alpha, beta)]\n",
    "    return np.argmax(sampled_theta)\n",
    "\n",
    "# Perform trial function\n",
    "def perform_trial(bandit_index):\n",
    "    bandit = bandit_names[bandit_index]\n",
    "    # Simulate a Bernoulli trial using the true probability of success\n",
    "    return stats.bernoulli(p=true_probabilities[bandit]).rvs(size=1)\n",
    "\n",
    "# Main experiment loop\n",
    "number_of_trials = 100  # Define the number of trials\n",
    "for t in range(number_of_trials):\n",
    "    chosen_bandit_index = choose_bandit()\n",
    "    result = perform_trial(chosen_bandit_index)\n",
    "    update_posterior(chosen_bandit_index, result)\n",
    "\n",
    "    print(f'After trial {t+1}, bandit {bandit_names[chosen_bandit_index]}: Alpha {alpha[chosen_bandit_index]}, Beta {beta[chosen_bandit_index]}, Expectation {round(alpha[chosen_bandit_index]/(alpha[chosen_bandit_index]+beta[chosen_bandit_index]),2)}')\n",
    "post_expectations = alpha/(alpha+beta)\n",
    "print(f'True/Posterior expectations: \"Red\" {true_probabilities[\"Red\"]}/{round(post_expectations[0],2)}, \"Blue\" {true_probabilities[\"Blue\"]}/{round(post_expectations[1],2)}, \"Other\" {true_probabilities[\"Other\"]}/{round(post_expectations[2],2)}')\n",
    "print(f'Bandit \"{bandit_names[np.argmax(post_expectations)]}\" has the highest posterior expectation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685d65f8-5c22-4ff4-b764-f332dac74917",
   "metadata": {},
   "source": [
    "The algorithm for the multi-armed Bayesian bandit problem begins by establishing a Beta distribution for each bandit with uninformative uniform priors, where both alpha and beta parameters are set to 1. This reflects an initial state of equal likelihood for success across all bandits. Trials are conducted where a bandit is chosen and its outcome, success or failure, is observed. These outcomes are based on predetermined probabilities unique to each bandit, simulating a Bernoulli process.\n",
    "\n",
    "After each trial, the algorithm updates its belief about the chosen bandit's probability of success. This is done by adjusting the bandit's Beta posterior distribution hyperparameters: alpha is increased by 1 for a success, and beta is increased by 1 for a failure. The decision on which bandit to select for each trial is based on sampling from the posterior distributions of all bandits. The bandit with the highest sampled posterior value is chosen, a strategy that balances exploration and exploitation.\n",
    "\n",
    "Over successive trials, the algorithm refines its understanding of each bandit's success probability. This continuous updating leads to more accurate predictions on the exploited bandit and, generally, a preference for the bandit with the highest actual probability of success if the true probabilities are not very close."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
